# -*- coding: utf-8 -*-
"""lab-3-Ruben-IMDB.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1N5oSD9sDY71NZ2gHpQAlSLnapLjuiok8

# The IMDB Dataset

---

### Colab Note

Don't forget that you can link your notebook to your drive and save your work there. Then you can download and backup your models, reload them to keep training them, or upload datasets to your drive.
"""

import os
import sys

if 'google.colab' in sys.modules:
    from google.colab import drive
    drive.mount('/content/drive')
    os.chdir('drive/My Drive/') # 'My Drive' is the default name of Google Drives
    os.listdir()

# use os.chdir("my-directory") # to change directory, and
# os.listdir()                 # to list its contents
# os.getcwd()                  # to get the name of the current directory
# os.mkdir("my-new-dir")       # to create a new directory
# See: https://realpython.com/working-with-files-in-python/

# You can also use bash commands directly, preceded by a bang
# !ls
# However, the following will *not* change the Python directory
# the notebook points to (use os.chdir for that)!
# !cd my-directory

"""---

## 1. Theory

Make sure you understand the first video of 3Blue1Brown's introduction to neural nets, and ask questions if there's anything unclear.
"""

from IPython.display import YouTubeVideo
YouTubeVideo('Ilg3gGewQ5U', width=853, height=480) # 3Blue1Brown 3

"""The fourth video is optional."""

YouTubeVideo('tIeHLnjs5U8', width=853, height=480) # 3Blue1Brown 4

"""---

## 2. Practice
"""

import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt

"""### For reproducible results

```python
tf.random.set_seed(42) # can be any number
```

### Experiments

- Experiment with only one layer, then with more (deeper net);
- Experiment with more or fewer hidden units â€“ 32 units, 64 units etc.
  - One nice challenge is to see how good your results get with a bigger network, then see if you can get to the same level with a smaller one by training longer, or tweaking the learning rate/changing the optimizer;
  - Another is to see how good a result you can get with a fixed number of epochs (e.g. 5);
- Experiment with replacing `relu` with `tanh` activations;
- Try the [`Adam` optimizer](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers#classes_2): `optimizers.Adam(learning_rate=0.001)`
- Investigate the effect of different learning rates;
- Investigate the effect of a smaller (or bigger) batch size;
- Train for more epochs, or, conversely, set a number of epochs (say 5), and see how good you can get your model in just these 5 epochs;

**Think about how to record and organise your experiments in a neat way!**
"""

def vectorize_sequences(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1.0
    return results


# load the IMDB dataset (max review length
(train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.imdb.load_data(num_words=10000)

# preprocess
x_train = vectorize_sequences(train_data)
x_test = vectorize_sequences(test_data)

y_train = np.asarray(train_labels).astype("float32")
y_test = np.asarray(test_labels).astype("float32")

# split training set into train & validation
partial_x_train = x_train[10000:]
partial_y_train = y_train[10000:]
x_val = x_train[:10000]
y_val = y_train[:10000]

# build
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Dense(16, activation="relu", input_shape=(10000,)))
model.add(tf.keras.layers.Dense(16, activation="relu"))
model.add(tf.keras.layers.Dense(1, activation="sigmoid"))

model.compile(
    optimizer="rmsprop",
    loss="binary_crossentropy",
    metrics=["accuracy"]
)

"""Sanity check, how does our network perform before training (use `.evaluate` on `partial_x_train, partial_y_train`). Is the accuracy a value you would expect?"""

model.evaluate(partial_x_train, partial_y_train)

"""## Baseline accuracy is 0.4990 - Goal is to get a better result than this.

Now we can train.
"""

# save data from training into the 'history' object
history = model.fit(
    partial_x_train,
    partial_y_train,
    epochs=20,
    batch_size=128,
    validation_data=(x_val, y_val),
)

"""### Visualise your results

Thanks to these plots, it is easier to spot the epoch (epoch number) where our net reached peak performance (lowest *validation loss*/highest *validation accuracy*, prioritising accuracy if the two are not the same).

Think about what would be a good strategy to keep your code as organised as possible as you run many experiments?
"""

history_dict = history.history
history_dict.keys()

history_dict["loss"]

loss = history_dict["loss"]
val_loss = history_dict["val_loss"]

epochs = range(1, len(loss) + 1)

blue_dots = "bo"
solid_blue_line = "b"

plt.plot(epochs, loss, blue_dots, label="Training loss")
plt.plot(epochs, val_loss, solid_blue_line, label="Validation loss")
plt.title("Training and validation loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()

plt.show()

history_dict["accuracy"]

plt.clf()

acc = history_dict["accuracy"]
val_acc = history_dict["val_accuracy"]

epochs = range(1, len(acc) + 1)

blue_dots = "bo"
solid_blue_line = "b"

plt.plot(epochs, acc, blue_dots, label="Training acc")
plt.plot(epochs, val_acc, solid_blue_line, label="Validation acc")
plt.title("Training and validation acc")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()

plt.show()

print("The best validation accuracy at epoch", np.argmax(history_dict["val_accuracy"]) + 1)

"""## Experiments"""

def plot_history(history):
    history_dict = history.history
    loss = history_dict["loss"]
    val_loss = history_dict["val_loss"]

    epochs = range(1, len(loss) + 1)

    blue_dots = "bo"
    solid_blue_line = "b"

    plt.plot(epochs, loss, blue_dots, label="Training loss")
    plt.plot(epochs, val_loss, solid_blue_line, label="Validation loss")
    plt.title("Training and validation loss")
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.legend()

    plt.show()

    acc = history_dict["accuracy"]
    val_acc = history_dict["val_accuracy"]

    epochs = range(1, len(acc) + 1)

    blue_dots = "bo"
    solid_blue_line = "b"

    plt.plot(epochs, acc, blue_dots, label="Training acc")
    plt.plot(epochs, val_acc, solid_blue_line, label="Validation acc")
    plt.title("Training and validation acc")
    plt.xlabel("Epochs")
    plt.ylabel("Accuracy")
    plt.legend()

    plt.show()

## Build function
def build_models():

    tf.keras.backend.clear_session()
    model = tf.keras.models.Sequential()
    model.add(tf.keras.layers.Dense(16, activation="relu", input_shape=(10000,)))
    model.add(tf.keras.layers.Dense(16, activation="relu"))
    model.add(tf.keras.layers.Dense(1, activation="sigmoid"))

    model.compile(
        optimizer="rmsprop",
        loss="binary_crossentropy",
        metrics=["accuracy"]
    )
    return model

"""## Network 1"""

for batch_size in [12, 64, 144, 336, 960, 1728]:

    print("Training with epoch 20 and batch size:", batch_size)

    # build
    model = build_models()

    # save data from training into the 'history' object
    history = model.fit(
        partial_x_train,
        partial_y_train,
        epochs=20,
        batch_size=batch_size,
        validation_data=(x_val, y_val),
    )

    plot_history(history)
    print()
    print("The max validation accuracy of", np.max(history.history["val_accuracy"]))
    print("At Epoch", np.argmax(history.history["val_accuracy"]) + 1)
    print("The max validation loss of", np.max(history.history["val_loss"]))
    print("At Epoch", np.argmax(history.history["val_loss"]) + 1)
    print("=" * 40)
    print()

"""## Experiment 1 Conclusion Results:
In this first Experiment I used epoch 20 and a list of bach sizes 12, 64, 144, 336, 960, 1728 and determined using the validation accuracy and loss that the bach size of 1728 (6th loop)produced the best accuracy validation of 0890500009059906 at Epoch 9 and worst validation loss of was produced by bach 12 (1st loop) at epoch 20 of 2.3072903156280518

## Network 2
"""

for batch_size in [12, 64, 144, 336, 960, 1728]:

    print("Training with epoch 8 and batch size:", batch_size)

    # build
    model = build_models()

    # save data from training into the 'history' object
    history = model.fit(
        partial_x_train,
        partial_y_train,
        epochs=8,
        batch_size=batch_size,
        validation_data=(x_val, y_val),
    )

    plot_history(history)
    print()
    print("The max validation accuracy of", np.max(history.history["val_accuracy"]))
    print("At Epoch", np.argmax(history.history["val_accuracy"]) + 1)
    print("The max validation loss of", np.max(history.history["val_loss"]))
    print("At Epoch", np.argmax(history.history["val_loss"]) + 1)
    print("=" * 40)
    print()

"""## Experiment 2 Conclusion Results:
In this second Experiment I used epoch 8 and a list of bach sizes 12, 64, 144, 336, 960, 1728 and determined using the validation accuracy and loss that the bach size of `960 (4th loop)produced the best accuracy validation of  0.8934000134468079 at Epoch 3` and worst validation loss of was produced by bach 1272 (6th loop) at epoch 1 of 0.5346906185150146

## Network 3
"""

for batch_size in [12, 64, 144, 336, 960, 1728]:

    print("Training with epoch 70 and batch size:", batch_size)

    # build
    model = build_models()

    # save data from training into the 'history' object
    history = model.fit(
        partial_x_train,
        partial_y_train,
        epochs=70,
        batch_size=batch_size,
        validation_data=(x_val, y_val),
    )

    plot_history(history)
    print()
    print("The max validation accuracy of", np.max(history.history["val_accuracy"]))
    print("At Epoch", np.argmax(history.history["val_accuracy"]) + 1)
    print("The max validation loss of", np.max(history.history["val_loss"]))
    print("At Epoch", np.argmax(history.history["val_loss"]) + 1)
    print("=" * 40)
    print()

"""## Experiment 3 Conclusion Results:
In the third experiment I used epoch 70 with a bacth of 12, 64, 144, 336, 960, 1728 and determined using the validation accuracy and loss that the bach size of 960 (5th loop) produced the best validation accuracy of 0.8906000256538391 at Epoch 7 and the worst validation loss was produced by batch 12 with a result of 2.950875759124756
At Epoch 70

## Network 4 With Regularisation
"""

## Build with regularisation
def build_model_reg(layer_1_units, layer_2_units, layer_3_units, reg=True,clear=True):
  if clear:
      tf.keras.backend.clear_session()
  model = tf.keras.models.Sequential()
  #Regularisation(only if reg is true)
  model.add(
      tf.keras.layers.Dense(
          layer_1_units,
          kernel_regularizer=tf.keras.regularizers.l2(0.002) if reg else None,
          activation='relu', input_shape=(10000,)
      )
  )

  model.add(
      tf.keras.layers.Dense(
          layer_2_units,
          kernel_regularizer=tf.keras.regularizers.l2(0.001) if reg else None,
          activation='relu'
      )
  )

  model.add(tf.keras.layers.Dense(layer_3_units, activation='sigmoid'))
  model.compile(
      optimizer='rmsprop',
      loss='binary_crossentropy',
      metrics=['accuracy']
  )
  return model

# I used the same epoch 7 of network 2 as it gave me the best result out of the 3 networks without regularisation.
for batch_size in [12, 64, 144, 336, 960, 1728]:

    print("Training with epoch 8, regularizers layer 1 (16 units) = 0.002 , regularizers layer 2 (16 units) = 0.001 and batch size:", batch_size)

    # build
    model = build_model_reg(16,16,1,True,True)

    # save data from training into the 'history' object
    history = model.fit(
        partial_x_train,
        partial_y_train,
        epochs=8,
        batch_size=batch_size,
        validation_data=(x_val, y_val),
    )

    plot_history(history)
    print()
    print("The max validation accuracy of", np.max(history.history["val_accuracy"]))
    print("At Epoch", np.argmax(history.history["val_accuracy"]) + 1)
    print("The max validation loss of", np.max(history.history["val_loss"]))
    print("At Epoch", np.argmax(history.history["val_loss"]) + 1)
    print("=" * 40)
    print()

"""## Experiment 4 Results:

In experiment 4 I used Regularization 0.002 in layer 1 and 0.001 in layer 2 and epoch 8 with a bacth of 12, 64, 144, 336, 960, 1728 and determined using the validation accuracy and loss that the bach size of 960 (5th loop) produced the best validation accuracy of 0.8884000182151794 at Epoch 5 and the worst validation loss was produced by batch 1728 with a result of  0.6643850803375244 At Epoch 1

## Network 5 With Regularisation
"""

## Build with regularisation
def build_model_reg(layer_1_units, layer_2_units, layer_3_units, reg=True,clear=True):
  if clear:
      tf.keras.backend.clear_session()
  model = tf.keras.models.Sequential()
  #Regularisation(only if reg is true)
  model.add(
      tf.keras.layers.Dense(
          layer_1_units,
          kernel_regularizer=tf.keras.regularizers.l2(0.002) if reg else None,
          activation='relu', input_shape=(10000,)
      )
  )

  model.add(
      tf.keras.layers.Dense(
          layer_2_units,
          kernel_regularizer=tf.keras.regularizers.l2(0.001) if reg else None,
          activation='relu'
      )
  )

  model.add(tf.keras.layers.Dense(layer_3_units, activation='sigmoid'))
  model.compile(
      optimizer='rmsprop',
      loss='binary_crossentropy',
      metrics=['accuracy']
  )
  return model

# I used the same epoch 7 of network 2 as it gave me the best result out of the 3 networks without regularisation.
for batch_size in [12, 64, 144, 336, 960, 1728]:

    print("Training with epoch 8, regularizers layer 1 (10 units) = 0.002 , regularizers layer 2 (5 units) = 0.001 and batch size:", batch_size)

    # build
    model = build_model_reg(10,5,1,True,True)

    # save data from training into the 'history' object
    history = model.fit(
        partial_x_train,
        partial_y_train,
        epochs=8,
        batch_size=batch_size,
        validation_data=(x_val, y_val),
    )

    plot_history(history)
    print()
    print("The max validation accuracy of", np.max(history.history["val_accuracy"]))
    print("At Epoch", np.argmax(history.history["val_accuracy"]) + 1)
    print("The max validation loss of", np.max(history.history["val_loss"]))
    print("At Epoch", np.argmax(history.history["val_loss"]) + 1)
    print("=" * 40)
    print()

"""## Experiment 5 Results:

In experiment 5 I used Regularization 0.002 in layer 1 (10 units) and 0.001 in layer 2 (5 units) and epoch 8 with a bacth of 12, 64, 144, 336, 960, 1728 and determined using the validation accuracy and loss that the bach size of 144 (3th loop) produced the best validation accuracy of 0.8898000121116638 at Epoch 2 and the worst validation loss was produced by batch 1728 with a result of 0.6702649593353271 At Epoch 1

## NETWORK 6 With Low Dropout
"""

# Build function with Low drop
def build_model_drop(layer1_units, layer2_units,layer3_units, drop, clear=True):
    if clear:
        tf.keras.backend.clear_session()
    model = tf.keras.models.Sequential()

    model.add(tf.keras.layers.Dense(layer1_units, activation='relu', input_shape=(10000,)))
    if drop:
      model.add(tf.keras.layers.Dropout(0.25))

    model.add(tf.keras.layers.Dense(layer2_units, activation='relu'))
    if drop:
      model.add(tf.keras.layers.Dropout(0.25))

    model.add(tf.keras.layers.Dense(layer3_units, activation='sigmoid'))
    model.compile(
      optimizer='rmsprop',
      loss='binary_crossentropy',
      metrics=['accuracy']
    )
    return model

# I used the same epoch 7 of network 2 as it gave me the best result out of the 3 networks without regularisation or dropout.
for batch_size in [12, 64, 144, 336, 960, 1728]:

    print("Training with epoch 8, droputs on  layer 1 and 2 of 0.25 and batch size:", batch_size)

    # build
    model = build_model_drop(16,16,1,True,True)

    # save data from training into the 'history' object
    history = model.fit(
        partial_x_train,
        partial_y_train,
        epochs=8,
        batch_size=batch_size,
        validation_data=(x_val, y_val),
    )

    plot_history(history)
    print()
    print("The max validation accuracy of", np.max(history.history["val_accuracy"]))
    print("At Epoch", np.argmax(history.history["val_accuracy"]) + 1)
    print("The max validation loss of", np.max(history.history["val_loss"]))
    print("At Epoch", np.argmax(history.history["val_loss"]) + 1)
    print("=" * 40)
    print()

"""## Experiment 6 Results:

In experiment 6 I used Drop of 0.25 in layer 1 and layer 2 I used epoch 8 with a bacth of 12, 64, 144, 336, 960, 1728 and determined using the validation accuracy and loss that the bach size of 64 (2th loop) produced the best validation accuracy of 0.8889999985694885 at Epoch 3 and the worst validation loss was produced by batch 1728 with a result of 0.5775330662727356 At Epoch 1

## NETWORK 7 with High dropout
"""

# Build function with High drop
def build_model_drop(layer1_units, layer2_units,layer3_units, drop, clear=True):
    if clear:
        tf.keras.backend.clear_session()
    model = tf.keras.models.Sequential()

    model.add(tf.keras.layers.Dense(layer1_units, activation='relu', input_shape=(10000,)))
    if drop:
      model.add(tf.keras.layers.Dropout(0.75))

    model.add(tf.keras.layers.Dense(layer2_units, activation='relu'))
    if drop:
      model.add(tf.keras.layers.Dropout(0.75))

    model.add(tf.keras.layers.Dense(layer3_units, activation='sigmoid'))
    model.compile(
      optimizer='rmsprop',
      loss='binary_crossentropy',
      metrics=['accuracy']
    )
    return model

# I used the same epoch 7 of network 2 as it gave me the best result out of the 3 networks without regularisation or dropout.
for batch_size in [12, 64, 144, 336, 960, 1728]:

    print("Training with epoch 8, droputs on  layer 1 and 2 of 0.75 and batch size:", batch_size)

    # build
    model = build_model_drop(16,16,1,True,True)

    # save data from training into the 'history' object
    history = model.fit(
        partial_x_train,
        partial_y_train,
        epochs=8,
        batch_size=batch_size,
        validation_data=(x_val, y_val),
    )

    plot_history(history)
    print()
    print("The max validation accuracy of", np.max(history.history["val_accuracy"]))
    print("At Epoch", np.argmax(history.history["val_accuracy"]) + 1)
    print("The max validation loss of", np.max(history.history["val_loss"]))
    print("At Epoch", np.argmax(history.history["val_loss"]) + 1)
    print("=" * 40)
    print()

"""## Experiment 7 Results:

In experiment 7 I used Drop of 0.75 in layer 1 and layer 2 I used epoch 8 with a bacth of 12, 64, 144, 336, 960, 1728 and determined using the validation accuracy and loss that the bach size of 144 (3th loop) produced the best validation accuracy of 0.8876000046730042 at Epoch 8 and the worst validation loss was produced by batch 1728 with a result of 0.6801914572715759 At Epoch 1

## Network 8 With Regularisation and Dropout
"""

# Build function with Low drop and 0.008 Reg
def build_model_drop_and_reg(layers=[11,44,77], reg=True, dropout_rate = 0.25, clear=True):
    if clear:
        tf.keras.backend.clear_session()
    model = tf.keras.Sequential()

    model.add(tf.keras.layers.Dense(16, kernel_regularizer=tf.keras.regularizers.l2(0.008) if reg else None, activation='relu', input_shape=(10000,)))

    for l in layers:
        model.add(tf.keras.layers.Dense(l,activation='relu'))

    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))
    model.add(tf.keras.layers.Dropout(dropout_rate))
    model.compile(
      optimizer='rmsprop',
      loss='binary_crossentropy',
      metrics=['accuracy']
    )
    return model

# I used the same epoch 7 of network 2 as it gave me the best result out of the 3 networks without regularisation or dropout.
for batch_size in [12, 64, 144, 336, 960, 1728]:

    print("Training with epoch 8, droputs on layers = 0.25,reg of 0.008 and batch size:", batch_size)

    # build
    model = build_model_drop_and_reg()

    # save data from training into the 'history' object
    history = model.fit(
        partial_x_train,
        partial_y_train,
        epochs=8,
        batch_size=batch_size,
        validation_data=(x_val, y_val),
    )

    plot_history(history)
    print()
    print("The max validation accuracy of", np.max(history.history["val_accuracy"]))
    print("At Epoch", np.argmax(history.history["val_accuracy"]) + 1)
    print("The max validation loss of", np.max(history.history["val_loss"]))
    print("At Epoch", np.argmax(history.history["val_loss"]) + 1)
    print("=" * 40)
    print()

"""## Experiment 8 Results:

In experiment 8 I used Drop of 0.25 and regularisation of 0.008 in layer 1  I used epoch 8 with a bacth of 12, 64, 144, 336, 960, 1728 and determined using the validation accuracy and loss that the bach size of 336 (4th loop) produced the best validation accuracy of 0.8781999945640564 at Epoch 8 and the worst validation loss was produced by batch 1728 with a result of 0.7676300406455994 At Epoch 1

## NETWORK 9 With Regularisation and Dropout
"""

# Build function with High drop and 0.015 Reg
def build_model_drop_and_reg(layers=[11,44,77], reg=True, dropout_rate = 0.75, clear=True):
    if clear:
        tf.keras.backend.clear_session()
    model = tf.keras.Sequential()

    model.add(tf.keras.layers.Dense(16, kernel_regularizer=tf.keras.regularizers.l2(0.015) if reg else None, activation='relu', input_shape=(10000,)))

    for l in layers:
        model.add(tf.keras.layers.Dense(l,activation='relu'))

    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))
    model.add(tf.keras.layers.Dropout(dropout_rate))
    model.compile(
      optimizer='rmsprop',
      loss='binary_crossentropy',
      metrics=['accuracy']
    )
    return model

# I used the same epoch 7 of network 2 as it gave me the best result out of the 3 networks without regularisation or dropout.
for batch_size in [12, 64, 144, 336, 960, 1728]:

    print("Training with epoch 8, droputs on layers = 0.75,reg of 0.015 and batch size:", batch_size)

    # build
    model = build_model_drop_and_reg()

    # save data from training into the 'history' object
    history = model.fit(
        partial_x_train,
        partial_y_train,
        epochs=8,
        batch_size=batch_size,
        validation_data=(x_val, y_val),
    )

    plot_history(history)
    print()
    print("The max validation accuracy of", np.max(history.history["val_accuracy"]))
    print("At Epoch", np.argmax(history.history["val_accuracy"]) + 1)
    print("The max validation loss of", np.max(history.history["val_loss"]))
    print("At Epoch", np.argmax(history.history["val_loss"]) + 1)
    print("=" * 40)
    print()

"""## Experiment 9 Results:

In experiment 9 I used Drop of 0.75 and regularisation of 0.008 in layer 1  I used epoch 8 with a bacth of 12, 64, 144, 336, 960, 1728 and determined using the validation accuracy and loss that the bach size of 336 (4th loop) produced the best validation accuracy of 0.5178999900817871 at Epoch 3 and the worst validation loss was produced by batch 1728 with a result of 0.8203943371772766 At Epoch 1

## 3. Conclusion

Take your best network and train on **all the training data** (`x_train`, `y_train`), without a train/validation split, using the same hyperparameters (optimizer, learning rate, network size, etc.) as your best run, for the optimal number of epochs (looking at your best validation curves).

## In conclusion:

The best network was network 2 as it gave the best accuracy validation at bach size of 960 (4th model)produced the best accuracy validation of  0.8934000134468079 at Epoch 3
"""

history = model.fit(
    x_train,
    y_train,
    epochs=8,
    batch_size=960,
)

"""Evaluate this last model on the test set (`x_test, y_test`)."""

model.evaluate(x_test,y_test)

"""### Use your model (optional)

Can you import the lecture code used to test the model on a review, and see if you agree with its prediction?

## I will be using a MNIST fashion model (External Data set)
"""

(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.fashion_mnist.load_data()

# MNIST

# load
(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.fashion_mnist.load_data()

# preprocess
train_images = train_images.reshape((60000, 28 * 28))
train_images = train_images.astype('float32') / 255
test_images = test_images.reshape((10000, 28 * 28))
test_images = test_images.astype('float32') / 255

train_labels_one_hot = tf.keras.utils.to_categorical(train_labels)
test_labels_one_hot = tf.keras.utils.to_categorical(test_labels)

train_labels[0], train_labels_one_hot[0]

# build
network = tf.keras.models.Sequential()
network.add(tf.keras.layers.Dense(512, activation='relu', input_shape=(28 * 28, )))
network.add(tf.keras.layers.Dense(10, activation='softmax'))
network.compile(
    optimizer='rmsprop',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

network.evaluate(test_images,test_labels_one_hot)

"""## The common sense baseline is 0.13% accuracy"""

# build
tf.keras.backend.clear_session()

network = tf.keras.models.Sequential()
network.add(tf.keras.layers.Dense(512, activation='relu', input_shape=(28 * 28, )))
network.add(tf.keras.layers.Dense(10, activation='softmax'))
network.compile(
    optimizer='rmsprop',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# train
network.fit(
    train_images,
    train_labels_one_hot,
    epochs=40,
    batch_size=128
)

# build
tf.keras.backend.clear_session()

network = tf.keras.models.Sequential()
network.add(tf.keras.layers.Dense(512, activation='relu', input_shape=(28 * 28, )))
network.add(tf.keras.layers.Dense(10, activation='softmax'))
network.compile(
    optimizer='rmsprop',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# train
network.fit(
    train_images,
    train_labels_one_hot,
    epochs=20,
    batch_size=128
)

# build
tf.keras.backend.clear_session()

network = tf.keras.models.Sequential()
network.add(tf.keras.layers.Dense(600, activation='relu', input_shape=(28 * 28, )))
network.add(tf.keras.layers.Dense(10, activation='softmax'))
network.compile(
    optimizer='rmsprop',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# train
network.fit(
    train_images,
    train_labels_one_hot,
    epochs=10,
    batch_size=128
)

# build
tf.keras.backend.clear_session()

network = tf.keras.models.Sequential()
network.add(tf.keras.layers.Dense(600, activation='relu', input_shape=(28 * 28, )))
network.add(tf.keras.layers.Dense(10, activation='softmax'))
network.compile(
    optimizer='rmsprop',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# train
network.fit(
    train_images,
    train_labels_one_hot,
    epochs=80,
    batch_size=128
)



tf.keras.backend.clear_session()

network = tf.keras.models.Sequential()
network.add(tf.keras.layers.Dense(550, activation='relu', input_shape=(28 * 28, )))
network.add(tf.keras.layers.Dense(10, activation='softmax'))
network.compile(
    optimizer='rmsprop',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# train
network.fit(
    train_images,
    train_labels_one_hot,
    epochs=80,
    batch_size=250
)

"""## Results of Experiments on Units:

| Model  | Accuracy | Loss |
| ---    | ---      | ---  |
| 1 layer,512 neurons, 20 epoch, batch 128|0.9374|0.1690|
| 1 layer,512 neurons, 40 epoch, batch 128|0.9603|0.1114|
| 1 layer,600 neurons, 10 epoch, batch 128|0.9137|0.2332|
| 1 layer,600 neurons, 80 epoch, batch 128|0.9814|0.0565|
| 1 layer,550 neurons, 80 epoch, batch 250|0.9822|0.0487|

The results with bigger the epoch the better the result,  the best result was the last experiment gave me the best result of 0.9822 accuracy
"""

tf.keras.backend.clear_session()

network = tf.keras.models.Sequential()
network.add(tf.keras.layers.Dense(550, activation='relu', input_shape=(28 * 28, )))
network.add(tf.keras.layers.Dense(10, activation='softmax'))
network.compile(
    optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001, momentum=0.0),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# train
network.fit(
    train_images,
    train_labels_one_hot,
    epochs=80,
    batch_size=250
)

tf.keras.backend.clear_session()

network = tf.keras.models.Sequential()
network.add(tf.keras.layers.Dense(550, activation='relu', input_shape=(28 * 28, )))
network.add(tf.keras.layers.Dense(10, activation='softmax'))
network.compile(
    optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.0001, momentum=0.0),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# train
network.fit(
    train_images,
    train_labels_one_hot,
    epochs=80,
    batch_size=250
)

tf.keras.backend.clear_session()

network = tf.keras.models.Sequential()
network.add(tf.keras.layers.Dense(550, activation='relu', input_shape=(28 * 28, )))
network.add(tf.keras.layers.Dense(10, activation='softmax'))
network.compile(
    optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.009, momentum=0.0),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# train
network.fit(
    train_images,
    train_labels_one_hot,
    epochs=80,
    batch_size=250
)

tf.keras.backend.clear_session()

network = tf.keras.models.Sequential()
network.add(tf.keras.layers.Dense(550, activation='relu', input_shape=(28 * 28, )))
network.add(tf.keras.layers.Dense(10, activation='softmax'))
network.compile(
    optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.075, momentum=0.0),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# train
network.fit(
    train_images,
    train_labels_one_hot,
    epochs=80,
    batch_size=250
)

tf.keras.backend.clear_session()

network = tf.keras.models.Sequential()
network.add(tf.keras.layers.Dense(550, activation='relu', input_shape=(28 * 28, )))
network.add(tf.keras.layers.Dense(10, activation='softmax'))
network.compile(
    optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.01, momentum=0.0),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# train
network.fit(
    train_images,
    train_labels_one_hot,
    epochs=80,
    batch_size=250
)

"""## Results of Experiments on optimizer:

Model Setup: `1 layer,550 neurons, 80 epoch, batch 250`

| Learning rate  | Accuracy | Loss |
| ---    | ---      | ---  |
| 0.001  |0.9826|0.0486|
| 0.0001  | 0.9349 |0.1901 |
| 0.009  | 0.9455 |0.1509 |
| 0.075  |0.7547 |0.7028 |
| 0.01  | 0.9379 |0.1708 |

The accuracy decreased throughout this experiment as none of the experiments passed the default

## Conclusion:

The best result that I got was in the default experiment with Optimizer 0.001 as the accuracy was 0.9826

### Save and load models

To save and load models locally, you can use [the high-level API](https://www.tensorflow.org/tutorials/keras/save_and_load):
```python
model.save("my_imdb_model.keras")
```
Later one, to reload it, use:
```python
reloaded_model = tf.keras.models.load_model('my_imdb_model.keras')
```

It is also possible to save not just the model, but also the state of your optimiser, and every variable used during training, using the morer involved [checkpoints](https://www.tensorflow.org/guide/checkpoint#create_the_checkpoint_objects).
"""